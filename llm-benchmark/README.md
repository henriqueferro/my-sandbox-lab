<p align="center">
  <h1>ğŸ§ª LLM Code Benchmark with DeepEval</h1>
</p>

This project benchmarks the code generation capabilities of leading Large Language Models (LLMs) using the [DeepEval](https://github.com/confident-ai/deepeval) evaluation library.

## ğŸ“¦ Supported Models

- ğŸ§  OpenAI GPT-4o
- ğŸ¤– Anthropic Claude Sonnet 4
- ğŸ¦™ Cohere Command
- ğŸš€ Grok-3 (x.ai)

## ğŸ“‚ Project Structure

```
llm-benchmark/
â”œâ”€â”€ main.py               # Main benchmarking script
â”œâ”€â”€ prompts/              # Evaluation prompts (one .txt per task)
â”‚   â”œâ”€â”€ fizzbuzz.txt
â”‚   â”œâ”€â”€ reverse_string.txt
â”‚   â”œâ”€â”€ sudoku_solver.txt
â”‚   â”œâ”€â”€ balanced_parentheses.txt
â”‚   â”œâ”€â”€ lru_cache.txt
â”‚   â”œâ”€â”€ find_all_anagrams.txt
â”‚   â””â”€â”€ dijkstra_shortest_path.txt
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md
```

## â–¶ï¸ How to Run

1. **Install dependencies:**
    ```bash
    cd llm-benchmark
    pip install -r requirements.txt
    ```

2. **Set your API keys:**
    ```bash
    export OPENAI_API_KEY="..."
    export ANTHROPIC_API_KEY="..."
    export COHERE_API_KEY="..."
    export XAI_API_KEY="..."
    ```

3. **Run the benchmark:**
    ```bash
    python main.py
    ```

## ğŸ“ How it works

- For each prompt in the `prompts/` folder, the script sends the same task to all supported models.
- Each model's output is evaluated using DeepEval's `AnswerRelevancyMetric`.
- The results (score per model per task) are printed to the console.

## ğŸ“ˆ Example Output

```
ğŸ” Benchmark: FizzBuzz
ğŸ—ï¸ Testando Grok-3 (x.ai)...
ğŸ“Š Resultado Grok-3 (x.ai): 1.00
ğŸ—ï¸ Testando Anthropic Claude Sonnet 4...
ğŸ“Š Resultado Anthropic Claude Sonnet 4: 1.00
ğŸ—ï¸ Testando Cohere Command...
ğŸ“Š Resultado Cohere Command: 0.92
ğŸ—ï¸ Testando OpenAI GPT-4o...
ğŸ“Š Resultado OpenAI GPT-4o: 1.00
```

## ğŸ“œ License

MIT License

---

**Contributions and suggestions are welcome!**
