# ğŸ§ª LLM Code Benchmark with DeepEval

This module compares the performance of LLM models on code generation tasks using the [DeepEval](https://github.com/confident-ai/deepeval) library.

## ğŸ“¦ Supported Models

- ğŸ§  OpenAI GPT-4o
- ğŸ¤– Anthropic Claude 3 Sonnet
- ğŸ¦™ Meta LLaMA 3 (via HuggingFace)

## ğŸ“‚ Structure

```
llm-benchmark/
â”œâ”€â”€ main.py               # Main benchmark code
â”œâ”€â”€ prompts/              # Evaluation prompts
â”‚   â”œâ”€â”€ fizzbuzz.txt
â”‚   â””â”€â”€ reverse_string.txt
â””â”€â”€ requirements.txt
```

## â–¶ï¸ How to run

```bash
cd llm-benchmark
pip install -r requirements.txt

# Set your keys:
export OPENAI_API_KEY="..."
export ANTHROPIC_API_KEY="..."
export HF_TOKEN="..."

# Run
python main.py
```

## ğŸ“ˆ Metric used

- `CodeMatchMetric`: compares logic, syntax, and semantics of the generated code.
